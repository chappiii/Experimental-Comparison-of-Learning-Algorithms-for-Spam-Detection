{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190fed90-9e8a-485b-a833-f0db7c04b633",
   "metadata": {},
   "source": [
    "## Setting up the environment, loading & splitting of the data, and class distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "901c4732-a1b2-42fc-ac50-65915d0eb1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution (%):\n",
      " 57\n",
      "0    60.595523\n",
      "1    39.404477\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load the dataset and split\n",
    "data = pd.read_csv(\"spambase.data\", header=None)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "n_splits = 10\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=5000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "class_distribution = y.value_counts(normalize=True) * 100\n",
    "print(\"Class Distribution (%):\\n\", class_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d2cc61-d26e-4e65-8495-e79f1affb60f",
   "metadata": {},
   "source": [
    "The class distribution is <mark>**60.6:39.4**</mark>, which indicates a mild imbalance, just meeting the threshold. Given the nature of the data—classified as Spam or Not Spam — <mark>**Recall**</mark> is a critical metric for evaluating model performance. This is because, in spam detection, minimizing false negatives (missed spam emails) is crucial to ensure a reliable system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc7fb5-1350-4c35-a088-f666d7ffe326",
   "metadata": {},
   "source": [
    "## Helper function definations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456e297-711e-46e9-a525-d432da801711",
   "metadata": {},
   "source": [
    "Following three helper functions is used to \n",
    "- Perform Stratified k-fold  \n",
    "- F1 score comparison \n",
    "- Average rank statistics to find the Friedman statistic\\n \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9dfcc1ff-85d9-4e8f-a985-7f9bfa17d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_k_fold_eval(model, model_name):\n",
    "    results = []\n",
    "    fold_metrics = {\"F1 Score\": [], \"Accuracy\": [], \"Training Time (s)\": []}\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y), start=1):\n",
    "        # Split data\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Measure training time\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "\n",
    "        # Predictions and metrics\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        # Store fold-specific metrics\n",
    "        fold_metrics[\"F1 Score\"].append(f1)\n",
    "        fold_metrics[\"Accuracy\"].append(accuracy)\n",
    "        fold_metrics[\"Training Time (s)\"].append(train_time)\n",
    "\n",
    "        # Store detailed results\n",
    "        results.append({\n",
    "            \"Fold\": fold_idx,\n",
    "            \"Training Time (s)\": train_time,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"F1 Score\": f1\n",
    "        })\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Compute Mean and Std\n",
    "    mean_row = {\n",
    "        \"Fold\": \"Mean\",\n",
    "        \"Training Time (s)\": results_df[\"Training Time (s)\"].mean(),\n",
    "        \"Accuracy\": results_df[\"Accuracy\"].mean(),\n",
    "        \"F1 Score\": results_df[\"F1 Score\"].mean()\n",
    "    }\n",
    "    std_row = {\n",
    "        \"Fold\": \"Std\",\n",
    "        \"Training Time (s)\": results_df[\"Training Time (s)\"].std(),\n",
    "        \"Accuracy\": results_df[\"Accuracy\"].std(),\n",
    "        \"F1 Score\": results_df[\"F1 Score\"].std()\n",
    "    }\n",
    "\n",
    "    # Add Mean and Std rows\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([mean_row, std_row])], ignore_index=True)\n",
    "\n",
    "    return results_df, fold_metrics\n",
    "\n",
    "\n",
    "def metric_comparison(precomputed_metrics, metric_name):\n",
    "    \"\"\"\n",
    "    Generate a comparison table for the given metric, including rankings for each fold.\n",
    "    \"\"\"\n",
    "    table = {\"Fold\": [f\"Fold {i}\" for i in range(1, n_splits + 1)]}\n",
    "    metric_results = {model_name: metrics[metric_name] for model_name, metrics in precomputed_metrics.items()}\n",
    "\n",
    "    for model_name, values in metric_results.items():\n",
    "        table[model_name] = []\n",
    "\n",
    "    for fold_idx in range(n_splits):\n",
    "        # Extract metric values for all models for this fold\n",
    "        fold_values = {model_name: metric_results[model_name][fold_idx] for model_name in metric_results}\n",
    "\n",
    "        # Rank models for this fold (lower rank is better)\n",
    "        sorted_models = sorted(fold_values.items(), key=lambda x: x[1], reverse=(metric_name != \"Training Time (s)\"))\n",
    "        rankings = {model: rank + 1 for rank, (model, _) in enumerate(sorted_models)}\n",
    "\n",
    "        # Append metric values with ranks\n",
    "        for model_name in metric_results.keys():\n",
    "            value = metric_results[model_name][fold_idx]\n",
    "            rank = rankings[model_name]\n",
    "            table[model_name].append(f\"{value:.4f} ({rank})\")\n",
    "\n",
    "    # Compute average rank for each model\n",
    "    avg_rank_row = [\"Avg Rank\"]\n",
    "    for model_name in metric_results.keys():\n",
    "        ranks = [int(val.split('(')[-1].strip(')')) for val in table[model_name]]\n",
    "        avg_rank = sum(ranks) / len(ranks)\n",
    "        avg_rank_row.append(f\"{avg_rank:.2f}\")\n",
    "\n",
    "    # Append average rank row to the table\n",
    "    table[\"Fold\"].append(avg_rank_row[0])\n",
    "    for idx, model_name in enumerate(metric_results.keys(), start=1):\n",
    "        table[model_name].append(avg_rank_row[idx])\n",
    "\n",
    "    # Convert table to DataFrame\n",
    "    results_df = pd.DataFrame(table)\n",
    "\n",
    "    # Print the table\n",
    "    print(f\"\\n{metric_name} Comparison:\\n\")\n",
    "    print(results_df.to_string(index=False))\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_rank_statistics(results_df, metric_name):\n",
    "    print(f\"\\nFriedman statistic calculation with metric:{metric_name}\")\n",
    "\n",
    "    # Extract the number of folds (n) and number of models (k)\n",
    "    n = len(results_df) - 1  # Exclude the last row (average ranks)\n",
    "    k = len(results_df.columns) - 1  # Exclude the \"Fold\" column\n",
    "\n",
    "    # Extract average ranks (last row excluding \"Fold\")\n",
    "    avg_ranks = results_df.iloc[-1, 1:].astype(float).values\n",
    "\n",
    "    # Calculate R_bar (average of the average ranks)\n",
    "    R_bar = np.mean(avg_ranks)\n",
    "\n",
    "    # Calculate N * SUM_j (Rj - R_bar)^2\n",
    "    N = n\n",
    "    term2 = N * np.sum((avg_ranks - R_bar) ** 2)\n",
    "\n",
    "    # Calculate (1 / (n - (k - 1))) * SUM_ij (Rij - Rj)^2\n",
    "    ranks_matrix = results_df.iloc[:n, 1:].map(lambda x: int(x.split('(')[-1].strip(')'))).values\n",
    "    term3 = (1 / (n * (k - 1))) * np.sum((ranks_matrix - R_bar) ** 2)\n",
    "\n",
    "    friedman_statistic = term2 / term3\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\tR̅ = {R_bar:.4f}\")\n",
    "    print(f\"\\tN * Σj (Rj - R̅)^2 = {term2:.4f}\")\n",
    "    print(f\"\\t(1 / (n * (k - 1))) * Σij (Rij - Rj)^2 = {term3:.4f}\")\n",
    "    print(f\"\\tFriedman statistic which is the ratio of the 2nd and 3nd terms = {term2:.4f}/{term3:.4f} = {friedman_statistic:.4f}\" )\n",
    "\n",
    "    # Update DataFrame with R_bar on the top bar\n",
    "    results_df.loc[-1] = [\"R_bar\"] + [f\"{R_bar:.4f}\"] * k\n",
    "    results_df.index = results_df.index + 1\n",
    "    results_df.sort_index(inplace=True)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31485deb-431b-417e-8883-fafa4bbb389c",
   "metadata": {},
   "source": [
    "# Procedure 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6cfb6f4-b9dd-4cb2-9a59-cbe8d18f737a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validatiaon results for Logistic Regression:\n",
      "\n",
      "Fold  Training Time (s)  Accuracy  F1 Score\n",
      "   1           3.998265  0.917570  0.895028\n",
      "   2           4.226130  0.923913  0.901961\n",
      "   3           6.156433  0.932609  0.913165\n",
      "   4           3.926541  0.936957  0.918768\n",
      "   5           3.548844  0.913043  0.890110\n",
      "   6           3.880666  0.936957  0.920110\n",
      "   7           4.809063  0.934783  0.915730\n",
      "   8           3.740062  0.930435  0.909605\n",
      "   9           2.888073  0.936957  0.915452\n",
      "  10           4.568816  0.915217  0.890141\n",
      "Mean           4.174289  0.927844  0.907007\n",
      " Std           0.875775  0.009581  0.011743\n",
      "\n",
      "Cross-validatiaon results for Random Forest:\n",
      "\n",
      "Fold  Training Time (s)  Accuracy  F1 Score\n",
      "   1           3.727035  0.952278  0.938889\n",
      "   2           3.686904  0.956522  0.944134\n",
      "   3           2.803832  0.960870  0.950000\n",
      "   4           3.336029  0.967391  0.957983\n",
      "   5           3.956414  0.945652  0.931129\n",
      "   6           2.648351  0.956522  0.944134\n",
      "   7           2.545018  0.958696  0.947368\n",
      "   8           2.875391  0.956522  0.943820\n",
      "   9           2.802648  0.941304  0.923513\n",
      "  10           2.270042  0.952174  0.937500\n",
      "Mean           3.065166  0.954793  0.941847\n",
      " Std           0.571747  0.007443  0.009720\n",
      "\n",
      "Cross-validatiaon results for XGBoost:\n",
      "\n",
      "Fold  Training Time (s)  Accuracy  F1 Score\n",
      "   1           1.007967  0.945770  0.931507\n",
      "   2           0.438933  0.958696  0.947075\n",
      "   3           0.413309  0.958696  0.948229\n",
      "   4           0.436984  0.960870  0.950000\n",
      "   5           0.437063  0.954348  0.942779\n",
      "   6           0.429228  0.956522  0.944444\n",
      "   7           0.419464  0.950000  0.936288\n",
      "   8           0.483608  0.958696  0.947945\n",
      "   9           0.473022  0.958696  0.947075\n",
      "  10           0.476989  0.945652  0.929178\n",
      "Mean           0.501657  0.954794  0.942452\n",
      " Std           0.179567  0.005666  0.007463\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation for all models and store metrics\n",
    "# Cross-validation report for selected models on given metric for 10 folds\n",
    "\n",
    "all_results = {}\n",
    "metrics_per_model = {}\n",
    "for model_name, model in models.items():\n",
    "    eval_results, metrics = stratified_k_fold_eval(model, model_name)\n",
    "    all_results[model_name] = eval_results\n",
    "    metrics_per_model[model_name] = metrics\n",
    "\n",
    "    print(f\"\\nCross-validatiaon results for {model_name}:\\n\")\n",
    "    print(eval_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e15b62-b639-454e-82a1-249f0d3f081c",
   "metadata": {},
   "source": [
    "# Procedure 3 Friedman test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9f7b195-0eb3-437f-b031-be9a7ef4c6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 Score Comparison:\n",
      "\n",
      "    Fold Logistic Regression Random Forest    XGBoost\n",
      "  Fold 1          0.8950 (3)    0.9389 (1) 0.9315 (2)\n",
      "  Fold 2          0.9020 (3)    0.9441 (2) 0.9471 (1)\n",
      "  Fold 3          0.9132 (3)    0.9500 (1) 0.9482 (2)\n",
      "  Fold 4          0.9188 (3)    0.9580 (1) 0.9500 (2)\n",
      "  Fold 5          0.8901 (3)    0.9311 (2) 0.9428 (1)\n",
      "  Fold 6          0.9201 (3)    0.9441 (2) 0.9444 (1)\n",
      "  Fold 7          0.9157 (3)    0.9474 (1) 0.9363 (2)\n",
      "  Fold 8          0.9096 (3)    0.9438 (2) 0.9479 (1)\n",
      "  Fold 9          0.9155 (3)    0.9235 (2) 0.9471 (1)\n",
      " Fold 10          0.8901 (3)    0.9375 (1) 0.9292 (2)\n",
      "Avg Rank                3.00          1.50       1.50\n",
      "\n",
      "Accuracy Comparison:\n",
      "\n",
      "    Fold Logistic Regression Random Forest    XGBoost\n",
      "  Fold 1          0.9176 (3)    0.9523 (1) 0.9458 (2)\n",
      "  Fold 2          0.9239 (3)    0.9565 (2) 0.9587 (1)\n",
      "  Fold 3          0.9326 (3)    0.9609 (1) 0.9587 (2)\n",
      "  Fold 4          0.9370 (3)    0.9674 (1) 0.9609 (2)\n",
      "  Fold 5          0.9130 (3)    0.9457 (2) 0.9543 (1)\n",
      "  Fold 6          0.9370 (3)    0.9565 (1) 0.9565 (2)\n",
      "  Fold 7          0.9348 (3)    0.9587 (1) 0.9500 (2)\n",
      "  Fold 8          0.9304 (3)    0.9565 (2) 0.9587 (1)\n",
      "  Fold 9          0.9370 (3)    0.9413 (2) 0.9587 (1)\n",
      " Fold 10          0.9152 (3)    0.9522 (1) 0.9457 (2)\n",
      "Avg Rank                3.00          1.40       1.60\n",
      "\n",
      "Training Time (s) Comparison:\n",
      "\n",
      "    Fold Logistic Regression Random Forest    XGBoost\n",
      "  Fold 1          3.9983 (3)    3.7270 (2) 1.0080 (1)\n",
      "  Fold 2          4.2261 (3)    3.6869 (2) 0.4389 (1)\n",
      "  Fold 3          6.1564 (3)    2.8038 (2) 0.4133 (1)\n",
      "  Fold 4          3.9265 (3)    3.3360 (2) 0.4370 (1)\n",
      "  Fold 5          3.5488 (2)    3.9564 (3) 0.4371 (1)\n",
      "  Fold 6          3.8807 (3)    2.6484 (2) 0.4292 (1)\n",
      "  Fold 7          4.8091 (3)    2.5450 (2) 0.4195 (1)\n",
      "  Fold 8          3.7401 (3)    2.8754 (2) 0.4836 (1)\n",
      "  Fold 9          2.8881 (3)    2.8026 (2) 0.4730 (1)\n",
      " Fold 10          4.5688 (3)    2.2700 (2) 0.4770 (1)\n",
      "Avg Rank                2.90          2.10       1.00\n"
     ]
    }
   ],
   "source": [
    "# Comparison between selected models based on given metrics for 10 folds\n",
    "f1_comparison_table = metric_comparison(metrics_per_model, \"F1 Score\")\n",
    "accuracy_comparison_table = metric_comparison(metrics_per_model, \"Accuracy\")\n",
    "training_time_comparison_table = metric_comparison(metrics_per_model, \"Training Time (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef8781ae-cb1a-48c4-b6f0-69f2c0d6ab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Friedman statistic calculation with metric:F1 Score\n",
      "\tR̅ = 2.0000\n",
      "\tN * Σj (Rj - R̅)^2 = 15.0000\n",
      "\t(1 / (n * (k - 1))) * Σij (Rij - Rj)^2 = 1.0000\n",
      "\tFriedman statistic which is the ratio of the 2nd and 3nd terms = 15.0000/1.0000 = 15.0000\n",
      "\n",
      "Friedman statistic calculation with metric:Accuracy\n",
      "\tR̅ = 2.0000\n",
      "\tN * Σj (Rj - R̅)^2 = 15.2000\n",
      "\t(1 / (n * (k - 1))) * Σij (Rij - Rj)^2 = 1.0000\n",
      "\tFriedman statistic which is the ratio of the 2nd and 3nd terms = 15.2000/1.0000 = 15.2000\n",
      "\n",
      "Friedman statistic calculation with metric:Training Time (s)\n",
      "\tR̅ = 2.0000\n",
      "\tN * Σj (Rj - R̅)^2 = 18.2000\n",
      "\t(1 / (n * (k - 1))) * Σij (Rij - Rj)^2 = 1.0000\n",
      "\tFriedman statistic which is the ratio of the 2nd and 3nd terms = 18.2000/1.0000 = 18.2000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Friedman statistic for different metrics\n",
    "updated_df = calculate_rank_statistics(f1_comparison_table, \"F1 Score\")\n",
    "updated_df = calculate_rank_statistics(accuracy_comparison_table, \"Accuracy\")\n",
    "updated_df = calculate_rank_statistics(training_time_comparison_table, \"Training Time (s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472cd321-7fb8-4617-8b34-e30cb53996c6",
   "metadata": {},
   "source": [
    "# F1 score Friedman test analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e15d5f-642f-4f20-bff2-d79b5d749fba",
   "metadata": {},
   "source": [
    "The critical value for k=3 and n=10 at the α=0.05 level is 6.20 (obtained from **Tables for the Friedman rank test**)\n",
    "Since the Friedman statistic (15) is greater than the critical value (6.2), we reject the **Null Hypothesis**. This suggests that at least one model is performing significantly differently for the given dataset but does not indicate which model is performing differently. Therefore, we conduct the Nemenyi test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5805c716-3d1d-4509-ab1c-3e051746027d",
   "metadata": {},
   "source": [
    "## Procedure 4 Nemenyi test for F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f4804-ec2b-4dc8-921c-e9779a0f92fe",
   "metadata": {},
   "source": [
    "to conduct Nemenyi test we compare the absolute difference of the average rank between each model and compare that with the critical difference value. to calculate the critical difference we obtained **qα** value of 2.343 for α=0.05, k=3 and the degree of freedom = infinity(obtained from **Studentized range q-table**)\n",
    "\n",
    "\n",
    "***we will be using this critical value for each metrice***)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f070a9-facf-44d9-b2f9-3502681f4f67",
   "metadata": {},
   "source": [
    "Critical difference calculation\n",
    "\n",
    "$CD = q_\\alpha \\cdot \\sqrt{\\frac{k \\cdot (k+1)}{6 \\cdot N}}$  \n",
    "$= 2.343 \\cdot \\sqrt{\\frac{3 \\cdot 4}{6 \\cdot 10}}$  \n",
    "$= 2.343 \\cdot 0.45$  \n",
    "$= 1.05$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b90c8e-2c8d-48d0-b3f3-e29d23164cf8",
   "metadata": {},
   "source": [
    "### Final Analysis\n",
    "For \\( R_j = [1.5, 3.0, 1.5] \\) and \\( CD = 1.05 \\), the output will be:\n",
    "\n",
    "| Comparison                          | Rank Difference | Significant |\n",
    "|---------------------|---------------|-------------------------------|\n",
    "|Logistic Regression Vs Random Forest | 1.50            | True        |\n",
    "|Random Forest Vs XGBoost             | 0.00            | False       |\n",
    "|Logistic Regression Vs XGBoost       | 1.50            | True        |\n",
    "\n",
    "### Compare each model\n",
    "\n",
    "1. **Logistic Regression vs Random Forest:**  \n",
    "   \\( |1.5 - 3.0| = 1.5 \\)  \n",
    "   Since \\( 1.5 > 1.05 \\), this is **significant**.\n",
    "\n",
    "2. **Random Forest Vs XGBoost:**  \n",
    "   \\( |1.5 - 1.5| = 0.0 \\)  \n",
    "   Since \\( 0.0 \\leq 1.05 \\), this is **not significant**\n",
    "\n",
    "3. **Logistic Regression** Vs **XGBoost**  \n",
    "   \\( |3.0 - 1.5| = 1.5 \\)  \n",
    "   Since \\( 1.5 > 1.05 \\), this is **significant**.\n",
    "\n",
    "This table summarizes the Nemenyi test results.\n",
    "\n",
    "***from the above Nemenyi test we conduct for F1 score we can conclude that Logistic Regression is preforming significantly different from the all the 3 models***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0dd9c-729c-42b2-8f39-0a9723ccf83d",
   "metadata": {},
   "source": [
    "# Accuracy Friedman test analysis¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b8793-9979-4326-9586-fee22915fbc5",
   "metadata": {},
   "source": [
    "Since the Friedman statistic (15.2) is greater than the critical value (6.2), we reject the **Null Hypothesis**. This suggests that at least one model is performing significantly differently for the given dataset but does not indicate which model is performing differently. Therefore, we conduct the Nemenyi test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5d0f33-9f99-4daa-9e8b-a673ff633e64",
   "metadata": {},
   "source": [
    "## Procedure 4 Nemenyi test for Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d164a-138b-4c3c-b2f0-4c5cfe2a33bf",
   "metadata": {},
   "source": [
    "### Final Analysis\n",
    "\n",
    "| Comparison                          | Rank Difference | Significant |\n",
    "|---------------------|---------------|-------------------------------|\n",
    "|Logistic Regression Vs Random Forest | 1.60            | True        |\n",
    "|Random Forest Vs XGBoost             | 0.20            | False       |\n",
    "|Logistic Regression Vs XGBoost       | 1.40            | True        |\n",
    "\n",
    "### Compare each model\n",
    "1. **Logistic Regression vs Random Forest:**  \n",
    "   \\( |1.4 - 3.0| = 1.6 \\)  \n",
    "   Since \\( 1.5 > 1.05 \\), this is **significant**.\n",
    "\n",
    "2. **Random Forest Vs XGBoost:**  \n",
    "   \\( |1.4 - 1.6| = 0.0 \\)  \n",
    "   Since \\( 0.20 \\leq 1.05 \\), this is **not significant**\n",
    "\n",
    "3. **Logistic Regression** Vs **XGBoost**  \n",
    "   \\( |3.0 - 1.6| = 1.4 \\)  \n",
    "   Since \\( 1.4 > 1.05 \\), this is **significant**.\n",
    "\n",
    "This table summarizes the Nemenyi test results.\n",
    "\n",
    "\n",
    "***from the above Nemenyi test we conduct for Accuracy we can conclude that Logistic Regression is preforming significantly  different from the all the 3 models***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f9f44-9c69-404d-825b-bd358f7c5d82",
   "metadata": {},
   "source": [
    "# Training Time Friedman test analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b6a5d-f184-472a-9b1b-7449065c0d11",
   "metadata": {},
   "source": [
    "Since the Friedman statistic (20) is greater than the critical value (6.2), we reject the **Null Hypothesis**. This suggests that at least one model is performing significantly differently for the given dataset but does not indicate which model is performing differently. Therefore, we conduct the Nemenyi test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7befe-cb04-43a4-9268-4cc2959efd02",
   "metadata": {},
   "source": [
    "## Procedure 4 Nemenyi test for Training Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6968d0a-2c8a-4f94-a3dd-04de00563463",
   "metadata": {},
   "source": [
    "### Final Analysis\n",
    "\n",
    "| Comparison                          | Rank Difference | Significant |\n",
    "|---------------------|---------------|-------------------------------|\n",
    "|Logistic Regression Vs Random Forest | 1.00            | True        |\n",
    "|Random Forest Vs XGBoost             | 1.00            | False       |\n",
    "|Logistic Regression Vs XGBoost       | 2.00            | True        |\n",
    "\n",
    "### Compare each model\n",
    "1. **Logistic Regression vs Random Forest:**  \n",
    "   \\( |3.0 - 2.0| = 1.0 \\)  \n",
    "   Since \\( 1 < 1.05 \\), this is ** not significant**.\n",
    "\n",
    "2. **Random Forest Vs XGBoost:**  \n",
    "   \\( |2.0 - 1.0| = 1.0 \\)  \n",
    "   Since \\( 1.0 < 1.05 \\), this is **not significant**\n",
    "\n",
    "3. **Logistic Regressionnt** Vs **XGBoost**  \n",
    "   \\( |3.0 - 1.0| = 2.0 \\)  \n",
    "   Since \\( 2.0 > 1.05 \\), this is **significant**.\n",
    "\n",
    "This table summarizes the Nemenyi test results.\n",
    "\n",
    "\n",
    "***from the above Nemenyi test we conduct for Training time we can conclude that Logistic Regression and XGBoost are preforming significantly different from each other***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474d4697-9e5d-4f76-9d4f-502cf8ad7eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
